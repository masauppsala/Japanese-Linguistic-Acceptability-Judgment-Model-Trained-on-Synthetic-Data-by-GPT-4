# Japanese-Linguistic-Acceptability-Judgment-Model-Trained-on-Synthetic-Data-by-GPT-4

## Script Overview for Japanese Linguistic Acceptability Judgment Models

### Introduction

This script is designed to evaluate the impact of synthetic data generated by GPT-4 on models trained for Japanese linguistic acceptability judgments. It integrates a sequence of operations from setting up the environment, preparing data, to training and evaluating deep learning models, specifically tailored for handling complex Japanese text data. This README outlines the purpose and functionality of each part of the script, providing a comprehensive guide to its use.

### Environment Setup

1. **Library Installation**:
   - Ensure all required Python libraries are installed, including `transformers`, `datasets`, `pandas`, and `numpy`. These libraries are essential for handling large datasets, performing tokenization, and manipulating data structures efficiently.

2. **Importing Modules**:
   - The script imports necessary components from the `transformers` library such as `AutoTokenizer` and `AutoModelForSequenceClassification`, which are pivotal for loading the pre-trained Japanese RoBERTa model and preparing it for sequence classification tasks.

### Data Preparation

3. **Data Loading**:
   - Training and evaluation datasets are automatically downloaded from specified URLs. These datasets are synthetic sentences providing a diverse range of linguistic structures for model training and testing.

4. **Data Shuffling**:
   - Datasets are shuffled to ensure that the order of data does not introduce bias into the training process. This is critical for maintaining the integrity of model evaluation.

5. **Tokenization and Dataset Formatting**:
   - Converts raw text datasets into a structured format that is suitable for input into the model. This includes converting sentences into token IDs and masks, which are necessary for the neural network to process the data.

### Model Training and Evaluation

6. **Balancing Data**:
   - Calculates class weights to address potential imbalances in the training data's label distribution. This prevents the model from favoring the majority class and helps in generalizing better across diverse inputs.

7. **Custom Trainer Configuration**:
   - A custom training class (`CustomTrainer`) is used, incorporating class weights into the loss function to enhance the model's learning efficacy, particularly on imbalanced datasets.

8. **Training Execution**:
   - The model is trained with finely tuned parameters, including adjustments in learning rate, batch sizes, and the number of epochs to optimize performance on the linguistic acceptability judgment task.

9. **Performance Evaluation**:
   - After training, the model is tested against a separate evaluation dataset to measure its ability to generalize to new, unseen data. This involves a detailed analysis of the model's performance across various syntactic phenomena, utilizing metrics like accuracy and Matthews correlation coefficient (MCC).

### Visualization and Reporting

10. **Results Output**:
   - Generates detailed reports and output files that document the modelâ€™s performance across different syntactic categories and overall. These results are crucial for analyzing the impact of synthetic data on model training and performance, providing empirical evidence to support research conclusions.
